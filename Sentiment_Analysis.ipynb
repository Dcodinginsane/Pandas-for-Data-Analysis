{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4bCwj89HogyJr1XBIVVPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dcodinginsane/Pandas-for-Data-Analysis/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large-Scale Sentiment Analysis with PySpark\n",
        "Comparative study of classification algorithms and feature extraction functions implemented in PySpark on 1,600,000 Tweets."
      ],
      "metadata": {
        "id": "tjQJe9jcoUbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYF1GESrfl20",
        "outputId": "11ff3d2d-4661-4798-ffeb-9e019e742c25"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4WCpP-YlgSxm",
        "outputId": "37098e43-c1cc-4bed-c2c6-ce6c016cdf1e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary imports\n",
        "import findspark\n",
        "findspark.init(\"/usr/local/lib/python3.10/dist-packages/pyspark\")\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
      ],
      "metadata": {
        "id": "ogW-Aj6lfacY"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark1 = SparkSession.builder\\\n",
        "            .master(\"local[*]\")\\\n",
        "            .appName(\"Twitter\")\\\n",
        "            .getOrCreate()"
      ],
      "metadata": {
        "id": "uY4uQMn7gaef"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "# Data schema\n",
        "schema = StructType([\n",
        "    StructField(\"target\", IntegerType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"query\", StringType(), True),\n",
        "    StructField(\"author\", StringType(), True),\n",
        "    StructField(\"tweet\", StringType(), True)])\n",
        "\n",
        "df = spark1.read.csv(path,\n",
        "                     inferSchema=True, # Spark uses the defined schema\n",
        "                     header=False,\n",
        "                     schema=schema)\n",
        "\n",
        "df.dropna()  # Drop rows containing NaN values for simplicity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB24PXPcgeqG",
        "outputId": "6b661d72-40fd-4091-820d-8b50991d2007"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[target: int, id: string, date: string, query: string, author: string, tweet: string]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the schema we’ve defined, our dataset contains 6 fields:\n",
        "\n",
        "* target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "* id: the id of the tweet (1235)\n",
        "* date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "* query: the API query used to get the data\n",
        "* author: the user that tweeted\n",
        "* tweet: the text of the tweet (what we are most interested in)"
      ],
      "metadata": {
        "id": "BUMmSlGChZu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def pre_process(text):\n",
        "    # Remove links\n",
        "    text = re.sub('http://\\S+|https://\\S+', '', text)\n",
        "    text = re.sub('http[s]?://\\S+', '', text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "    # Convert HTML references\n",
        "    text = re.sub('&amp', 'and', text)\n",
        "    text = re.sub('&lt', '<', text)\n",
        "    text = re.sub('&gt', '>', text)\n",
        "    #text = re.sub('\\xao', '', text)\n",
        "\n",
        "\n",
        "    # Remove new line characters\n",
        "    text = re.sub('[\\r\\n]+', ' ', text)\n",
        "\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Remove multiple space characters\n",
        "    text = re.sub('\\s+',' ', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "Gwc7uKEShJDc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "df = df.withColumn(\"AddCol\",F.when(F.col(\"tweet\").like(\"3\"),\"three\").otherwise(\"notthree\"))"
      ],
      "metadata": {
        "id": "BxohNtzB8Qfn"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "hT6A6XZZl4i9"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")"
      ],
      "metadata": {
        "id": "XMD8hTi8mCes"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtf = HashingTF(inputCol=\"words\", outputCol='tf')"
      ],
      "metadata": {
        "id": "HC_KSx2CmHFa"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf = IDF(inputCol='tf', outputCol=\"features\")"
      ],
      "metadata": {
        "id": "_6Kqk0BHmJid"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")"
      ],
      "metadata": {
        "id": "pqAGp7kOmMCk"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])"
      ],
      "metadata": {
        "id": "FN5ES7cPmPhP"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_trigrams(inputCol=[\"tweet\",\"target\"], n=3):\n",
        "\n",
        "    tokenizer = [Tokenizer(inputCol=\"tweet\", outputCol=\"words\")]\n",
        "\n",
        "    ngrams = [\n",
        "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "\n",
        "    cv = [\n",
        "        CountVectorizer(vocabSize=2**14,inputCol=\"{0}_grams\".format(i),\n",
        "            outputCol=\"{0}_tf\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "\n",
        "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
        "\n",
        "    assembler = [VectorAssembler(\n",
        "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
        "        outputCol=\"rawFeatures\"\n",
        "    )]\n",
        "\n",
        "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
        "\n",
        "    selector = [ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")]\n",
        "\n",
        "    lr = [LogisticRegression()]\n",
        "\n",
        "    return Pipeline(stages=tokenizer + ngrams + cv + idf + assembler + label_stringIdx + selector + lr)"
      ],
      "metadata": {
        "id": "SU5wuck1mVC9"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time time.sleep(5)\n",
        "from datetime import datetime\n",
        "\n",
        "datetime.utcnow()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "st = datetime.utcnow()\n",
        "pipelineFit = pipeline.fit(train_set)\n",
        "print('Training time:', datetime.utcnow() - st)\n",
        "\n",
        "predictions = pipelineFit.transform(test_set)\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "8FyG8S5NmYot",
        "outputId": "9877732d-cb4f-4001-b2b6-c814ca165664"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26.5 ms, sys: 341 µs, total: 26.8 ms\n",
            "Wall time: 5 s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-2e92622d86b3>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpipelineFit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "st = datetime.utcnow()\n",
        "pipelineFit = pipeline.fit(train_set)\n",
        "print('Training time:', datetime.utcnow() - st)\n",
        "\n",
        "predictions = pipelineFit.transform(test_set)\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "gQHOGVpRA3WJ",
        "outputId": "d2d14c11-9480-4af0-9158-bda182bc2579"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'utcnow'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.mllib.util import MLUtils\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel"
      ],
      "metadata": {
        "id": "_ym9xttU81Gp"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
        "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
        "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "\n",
        "lr = LogisticRegression()\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
        "    .addGrid(lr.maxIter, [20, 50, 100, 500, 1000]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10)"
      ],
      "metadata": {
        "id": "jiCV0c_cmdcy"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "pipelineFit = cv.fit(train_set)\n",
        "\n",
        "predictions = pipelineFit.transform(test_set)\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "bestModel = pipelineFit.bestModel\n",
        "pipelineFit.getEstimatorParamMaps()[np.argmax(pipelineFit.avgMetrics)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "KlXTipCImhby",
        "outputId": "be4f6ae5-0e45-4a54-cce3-a85fd388d6d3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "plt.plot(pipelineFit1.stages[-1].summary.roc.select('FPR').collect(),\n",
        "         pipelineFit1.stages[-1].summary.roc.select('TPR').collect(),\n",
        "        label=\"AUC=\" +str(round(pipelineFit1.stages[-1].summary.areaUnderROC, 3)))\n",
        "\n",
        "plt.plot(pipelineFit2.stages[-1].summary.roc.select('FPR').collect(),\n",
        "         pipelineFit2.stages[-1].summary.roc.select('TPR').collect(),\n",
        "        label=\"AUC=\" +str(round(pipelineFit2.stages[-1].summary.areaUnderROC, 3)))\n",
        "\n",
        "plt.plot(pipelineFit3.stages[-1].summary.roc.select('FPR').collect(),\n",
        "         pipelineFit3.stages[-1].summary.roc.select('TPR').collect(),\n",
        "        label=\"AUC=\" +str(round(pipelineFit3.stages[-1].summary.areaUnderROC, 3)))\n",
        "\n",
        "plt.plot(pipelineFit4.stages[-1].summary.roc.select('FPR').collect(),\n",
        "         pipelineFit4.stages[-1].summary.roc.select('TPR').collect(),\n",
        "        label=\"AUC=\" +str(round(pipelineFit4.stages [-1].summary.areaUnderROC, 3)))\n",
        "\n",
        "\n",
        "plt.plot(pipelineFit5.stages[-1].summary.roc.select('FPR').collect(),\n",
        "         pipelineFit5.stages[-1].summary.roc.select('TPR').collect(),\n",
        "        label=\"AUC=\" +str(round(pipelineFit5.stages[-1].summary.areaUnderROC, 3)))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"r--\", label=\"Guess\")\n",
        "\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UQTboUTUmn8A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}